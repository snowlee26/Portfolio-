{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis: Part 1, AWS and EMP\n",
    "### Collecting data from Amazon Web Service using Elastic Map Reducer.\n",
    "\n",
    "## Background\n",
    "\n",
    "We are working with a government health agency to create a suite of smart phone medical apps for use by aid workers in\n",
    "developing countries. This suite of apps will enable the aid workers to manage local health conditions by facilitating\n",
    "communication with medical professionals located elsewhere. The government agency requires that the app suite be \n",
    "bundled with one model of smart phone. This will help them to limit purchase costs and ensure uniformity when training\n",
    "aid workers to use the device. \n",
    "\n",
    "## Objective \n",
    "\n",
    "We were given a short list of devices that are all capable of executing the app suite's functions, and we were asked \n",
    "to examine the prevalence of positive and negative attitudes toward these devices on the web. Our goal is to narrow \n",
    "this list down to one device by conducting a broad-based web sentiment analysis to gain insight into the attitudes \n",
    "toward the devices.\n",
    "\n",
    "For the first part of the project, we will use the AWS Elastic Map Reduce (EMR) platform to run a series of Hadoop \n",
    "Streaming jobs that will collect large amounts of smart phone-related web pages from a massive repository of web data \n",
    "called the Common Crawl. Once this data has been gathered we will compile it into data matrix files for analysis. \n",
    "\n",
    "## Collect and Prepare the Data \n",
    "\n",
    "### General Approach\n",
    "\n",
    "Our general approach to this project is to count words associated with sentiment toward these devices within relevant \n",
    "documents on the web. We then leverage this data and machine learning methods to look for patterns in the documents \n",
    "that enable us to label each of these documents with a value that represents the level of positive or negative \n",
    "sentiment toward each of these devices. We then analyze and compare the frequency and distribution of the sentiment \n",
    "for each of these devices.\n",
    "\n",
    "In order to really gauge the sentiment toward these devices, we must do this on a very large scale. To that end, we \n",
    "use the cloud computing platform provided by Amazon Web Services (AWS) to conduct the analysis. The datasets we \n",
    "analyze will come from Common Crawl, an open repository of web crawl data that is stored on Amazon’s Public Data Sets.\n",
    "\n",
    "We are provided with \n",
    " \n",
    "* Mapper Python script (Mapper.py), to examines and counts data from portions of the Common Crawl data.\n",
    "[Mapper](https://github.com/snowlee26/Portfolio-/blob/master/Mapper.py)\n",
    "* Reducer Python script (Reducer.py), to accumulates the analysis from the individual mapper jobs.\n",
    "[Reducer](https://github.com/snowlee26/Portfolio-/blob/master/Reducer.py)\n",
    "* Aggregation script (Concatenatepv3.py), to helps stitch together the raw output from the multiple job flows.\n",
    "[Concatenate](https://github.com/snowlee26/Portfolio-/blob/master/concatenatepv3.py)\n",
    "\n",
    "### Identify the data source - Common Crawl\n",
    "\n",
    "Common Crawl crawls and archives the entire readable Internet once per month. The archived files are stored on Amazon \n",
    "Web Services N. Virginia S3. The crawl is split into 1000’s of roughly similar sized files which are then saved as \n",
    "WARC file type and gzipped (WARC stands for Web ARChive format). Each of these files has it’s own specific address and\n",
    "we use these addresses as input with Amazon Web Services.\n",
    "\n",
    "Because we are interested in sentiment mining, we will focus on using a subset of the WARC files that only contain \n",
    "text: WET. As a first step to getting our input addresses, we download the wet paths file for last month on [Common Crawl Blog](http://commoncrawl.org/connect/blog/).\n",
    "\n",
    "* The wet.paths file consists 10s of thousands of addresses like below, and it was saved as BDF file. \n",
    "![](wet.paths.png)\n",
    "* We added “s3://commoncrawl/” to the beginning of all the file addresses we intend to use as input, so that EWR will\n",
    "recognize the addresses. \n",
    "![](wet.paths.aws.png)\n",
    "* Set up three S3 buckets, a bucket that you will use for mapper and reducer scripts, a bucket for your output, and a \n",
    "bucket for debugging logs.  \n",
    "\n",
    "### Run the EMR job flow using AWS CLI(Command Line Interface)\n",
    "\n",
    "Since we are running very large jobs, we will use the command line interface for this task. This interface will be \n",
    "accessed from the 'terminal' in Mac OSX and 'Command Prompt' on a Windows machine. The Command Line Interface (CLI) in\n",
    "AWS provide the ability to programmatically launch and monitor progress of running job flows.\n",
    "\n",
    "We are provided with a CreateJson Python File (createJsonFiles.py). To run this file, we use the s3 addresses from the\n",
    "BDF file and it creates JSON files in the end. \n",
    "[CreateJson](https://github.com/snowlee26/Portfolio-/blob/master/createJsonFilesPv3.py)\n",
    "\n",
    "Steps to run the job:\n",
    "\n",
    "* Select WET file addresses. \n",
    "* Copy the CreateJsonFiles.py Python script into the folder with the .bdf file and personalize the script.\n",
    "  * Update the python script to have the correct S3 locations for your Mapper.py and Reducer.py files.\n",
    "  * Update the python script to have the correct S3 address for your output bucket.\n",
    "* Run the CreateJsonFiles.py Python File from the command shell to generate your json file.\n",
    "  * code\n",
    "![](code.png)\n",
    "  * Output of the code:\n",
    "![](json.png)\n",
    "* Checking the validity of your .json file. In order for the CLI to correctly process, json file has to be formatted \n",
    "corrected or be structured in the correct manner. Here, we used [JSONLint](https://jsonlint.com/) to validate our file.\n",
    "We did a little adjustment on the formating and now we are ready for the next step. \n",
    "* Run the the .json files from CLI to create a EMR Cluster.\n",
    "  * code\n",
    "    ![](createEMRcluster.png)\n",
    "* Monitor your Cluster from AWS Console.\n",
    "\n",
    "### Consolidate the results of the jobs\n",
    "\n",
    "We need to aggregate the results of the streaming jobs we set up. This involves the following steps:\n",
    "* Download the EMR output from S3 output bucket. We use CyberDuck to download all of the individual output folders to \n",
    "a single folder locates on the local machine. \n",
    "* Put concatenatepv3.py file in the same folder where we saved all the EMR output folders. Concatenatepv3.py will open\n",
    "each of the EMR output folders and aggregate all of the part files into two .csv files.\n",
    "* Open a command prompt and run the concatenatepv3.py script.\n",
    "![](concatenatepv3.png)\n",
    "* Rename 'concatenated_factors.csv' to LargeMatrix.csv for the machine learning process in the next step. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Dataset\n",
    "**Here, we show the first few rows of the final dataset we collected from AWS.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>iphone</th>\n",
       "      <th>samsunggalaxy</th>\n",
       "      <th>sonyxperia</th>\n",
       "      <th>nokialumina</th>\n",
       "      <th>htcphone</th>\n",
       "      <th>ios</th>\n",
       "      <th>googleandroid</th>\n",
       "      <th>iphonecampos</th>\n",
       "      <th>samsungcampos</th>\n",
       "      <th>...</th>\n",
       "      <th>samsungperunc</th>\n",
       "      <th>sonyperunc</th>\n",
       "      <th>nokiaperunc</th>\n",
       "      <th>htcperunc</th>\n",
       "      <th>iosperpos</th>\n",
       "      <th>googleperpos</th>\n",
       "      <th>iosperneg</th>\n",
       "      <th>googleperneg</th>\n",
       "      <th>iosperunc</th>\n",
       "      <th>googleperunc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  iphone  samsunggalaxy  sonyxperia  nokialumina  htcphone  ios  \\\n",
       "0   0       2              0           0            0         0    2   \n",
       "1   1       1              0           0            0         0    0   \n",
       "2   2       1              0           0            0         0    0   \n",
       "3   3       1              0           0            0         0    0   \n",
       "4   4       1              0           0            0         0    0   \n",
       "\n",
       "   googleandroid  iphonecampos  samsungcampos  ...  samsungperunc  sonyperunc  \\\n",
       "0              6             2              0  ...              0           0   \n",
       "1              0             0              0  ...              0           0   \n",
       "2              0             0              0  ...              0           0   \n",
       "3              0             0              0  ...              0           0   \n",
       "4              0             0              0  ...              0           0   \n",
       "\n",
       "   nokiaperunc  htcperunc  iosperpos  googleperpos  iosperneg  googleperneg  \\\n",
       "0            0          0          0             0          0             0   \n",
       "1            0          0          0             0          0             0   \n",
       "2            0          0          0             0          0             0   \n",
       "3            0          0          0             0          0             0   \n",
       "4            0          0          0             0          0             0   \n",
       "\n",
       "   iosperunc  googleperunc  \n",
       "0          0             0  \n",
       "1          0             0  \n",
       "2          0             0  \n",
       "3          0             0  \n",
       "4          0             0  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "LargeMatrix = pd.read_csv('100wet.csv')\n",
    "LargeMatrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we finished the first part of the project. We collected and created a dataset containing the level of positive \n",
    "or negative sentiment toward each of these devices. We will use this final dataset for analysis in the next step.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
